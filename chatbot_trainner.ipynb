{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0lXghNm57Kil"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum sentence length\n",
        "MAX_LENGTH = 13\n"
      ],
      "metadata": {
        "id": "tljRz9z_mCtc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'cornell_movie_dialogs.zip',\n",
        "    origin=\n",
        "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_dataset = os.path.join(\n",
        "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
        "\n",
        "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
        "path_to_movie_conversations = os.path.join(path_to_dataset,\n",
        "                                           'movie_conversations.txt')"
      ],
      "metadata": {
        "id": "zqt0z3k0nd_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5a257c-f679-4685-8951-bb339dc2954c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "9920512/9916637 [==============================] - 0s 0us/step\n",
            "9928704/9916637 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "  sentence = sentence.lower().strip()\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "  # removing contractions\n",
        "  sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
        "  sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
        "  sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
        "  sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
        "  sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
        "  sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
        "  sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
        "  sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
        "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "  sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
        "  sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
        "  sentence = re.sub(r\"n't\", \" not\", sentence)\n",
        "  sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
        "  sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "c4ownN9NmCk2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_conversations():\n",
        "  # dictionary of line id to text\n",
        "  id2line = {}\n",
        "  with open(path_to_movie_lines, errors='ignore') as file:\n",
        "    lines = file.readlines()\n",
        "  for line in lines:\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "    id2line[parts[0]] = parts[4]\n",
        "\n",
        "  inputs, outputs = [], []\n",
        "  with open(path_to_movie_conversations, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "  for line in lines:\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "    # get conversation in a list of line ID\n",
        "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]  #here [1: -1] is used to remove [,],' from str. Ex: ['L22', 'L33']\n",
        "    for i in range(len(conversation) - 1):\n",
        "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
        "      outputs.append('<START> ' + preprocess_sentence(id2line[conversation[i + 1]]) + ' <END>')\n",
        "  return inputs, outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T-Fm-zxEnu1q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "questions, answers = load_conversations()\n",
        "\n",
        "questions_short, answers_short = [], []\n",
        "for (que, ans) in zip(questions, answers):\n",
        "  if len(text_to_word_sequence(que)) <= MAX_LENGTH and len(text_to_word_sequence(ans)) <= MAX_LENGTH:\n",
        "      questions_short.append(que)\n",
        "      answers_short.append(ans)\n",
        "questions_short = questions_short[:15000]\n",
        "answers_short = answers_short[:15000]\n",
        "print(\"Questions in dataset: {}\".format(len(questions_short)))\n",
        "print(\"Answers in dataset: {}\".format(len(answers_short)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiWeE77xoroY",
        "outputId": "705b2e26-a67b-44bf-aace-0cd53ad2d1ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Questions in dataset: 15000\n",
            "Answers in dataset: 15000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(questions_short + answers_short)\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size : {}'.format(VOCAB_SIZE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR_qILDuSZrk",
        "outputId": "7ac8b8ed-0880-4401-9b27-c1858edad713"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size : 8619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "#with open('tokenizer.pickle', 'rb') as handle:\n",
        "#    tokenizera = pickle.load(handle)"
      ],
      "metadata": {
        "id": "EDAaCpoSxAkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_questions = tokenizer.texts_to_sequences(questions_short)\n",
        "\n",
        "encoder_input_data = pad_sequences(tokenized_questions, \n",
        "                                 maxlen=MAX_LENGTH,\n",
        "                                 padding='post')\n",
        "print(encoder_input_data.shape)\n",
        "\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers_short)\n",
        "decoder_input_data = pad_sequences(tokenized_answers,   \n",
        "                                   maxlen=MAX_LENGTH,\n",
        "                                   padding='post')\n",
        "print(decoder_input_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmcFWegKoAoR",
        "outputId": "0ccc107d-8b67-49cc-d28b-fa155e21e211"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15000, 13)\n",
            "(15000, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the first 'start' word from every answer\n",
        "decoder_output = []\n",
        "for i in range(len(tokenized_answers)):\n",
        "    decoder_output.append(tokenized_answers[i][1:])\n",
        "padded_answers = pad_sequences(decoder_output, maxlen=MAX_LENGTH, padding='post')\n",
        "decoder_output_data = to_categorical(padded_answers, VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "-8A62iEy4M_F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "4Lrv4Wa6LOAQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_inputs = Input(shape=(None,))\n",
        "enc_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(enc_inputs)\n",
        "enc_outputs, state_h, state_c = LSTM(200, return_state=True)(enc_embedding)\n",
        "enc_states = [state_h, state_c]\n",
        "# decoder will be used to capture space-dependent relations \n",
        "# between words from the answers using encoder's \n",
        "# internal state as a context\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(dec_inputs)\n",
        "dec_lstm = LSTM(200, return_state=True, return_sequences=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding,  \n",
        "                             initial_state=enc_states)\n",
        "# decoder is connected to the output Dense layer\n",
        "dec_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
        "output = dec_dense(dec_outputs)\n",
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "# output of this network will look like this:\n",
        "# y_true = [0.05, 0.95, 0...]\n",
        "# and expected one-hot encoded output like this:\n",
        "# y_pred = [0, 1, 0...]\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "5jyW37yD70Qn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ae880e-acf9-441a-9507-02b2372221f1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 200)    1723800     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 200)    1723800     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 200),        320800      ['embedding[0][0]']              \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 200),  320800      ['embedding_1[0][0]',            \n",
            "                                 (None, 200),                     'lstm[0][1]',                   \n",
            "                                 (None, 200)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 8619)   1732419     ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,821,619\n",
            "Trainable params: 5,821,619\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_output_data,  batch_size=16,  epochs=300)"
      ],
      "metadata": {
        "id": "A2R2s_FaAmAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('chatbot.h5')"
      ],
      "metadata": {
        "id": "20WRHQDXPkcw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inference_models():\n",
        "    dec_state_input_h = Input(shape=(200,))\n",
        "    dec_state_input_c = Input(shape=(200,))\n",
        "    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "    # these state vectors are used as an initial state \n",
        "    # for LSTM layer in the inference decoder\n",
        "    # third input is the Embedding layer as explained above   \n",
        "    dec_outputs, state_h, state_c = dec_lstm(dec_embedding,\n",
        "                                    initial_state=dec_states_inputs)\n",
        "    dec_states = [state_h, state_c]\n",
        "    # Dense layer is used to return OHE predicted word\n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "    dec_model = Model(\n",
        "        inputs=[dec_inputs] + dec_states_inputs,\n",
        "        outputs=[dec_outputs] + dec_states)\n",
        "   \n",
        "    # single encoder input is a question, represented as a sequence \n",
        "    # of integers padded with zeros\n",
        "    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n",
        "   \n",
        "    return enc_model, dec_model\n",
        "enc_model, dec_model = make_inference_models()"
      ],
      "metadata": {
        "id": "vq7kdORdmRYB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print(VOCAB_SIZE)\n",
        "enc_model = load_model('enc_model.h5')\n",
        "dec_model = load_model('dec_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y-w8yDFmJS1",
        "outputId": "58d431fc-d80b-4da4-8f5a-de4d53f26a22"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8619\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def str_to_tokens(sentence: str):\n",
        "    # convert input string to lowercase, \n",
        "    # then split it by whitespaces\n",
        "    words = sentence.lower().split()\n",
        "    # and then convert to a sequence \n",
        "    # of integers padded with zeros\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "    return pad_sequences([tokens_list], maxlen=MAX_LENGTH, padding='post')"
      ],
      "metadata": {
        "id": "K8y49UCeGA7U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "states_values = enc_model.predict(str_to_tokens(input('You : ')))\n",
        "empty_target_seq = np.zeros((1, 1))\n",
        "empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "stop_condition = False\n",
        "decoded_translation = ''\n",
        "while not stop_condition:\n",
        "    # feed the state vectors and 1-word target sequence \n",
        "    # to the decoder to produce predictions for the next word\n",
        "    dec_outputs, h, c = dec_model.predict([empty_target_seq] \n",
        "                                          + states_values)         \n",
        "    # sample the next word using these predictions\n",
        "    sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "    sampled_word = None\n",
        "    # append the sampled word to the target sequence\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if sampled_word_index == index:\n",
        "            if word != 'end':\n",
        "                decoded_translation += ' {}'.format(word)\n",
        "            sampled_word = word\n",
        "    # repeat until we generate the end-of-sequence word 'end' \n",
        "    # or we hit the length of answer limit\n",
        "    if sampled_word == 'end' or len(decoded_translation.split()) > MAX_LENGTH:\n",
        "        stop_condition = True\n",
        "    # prepare next iteration\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = sampled_word_index\n",
        "    states_values = [h, c]\n",
        "print(\"Bot: \" + decoded_translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydAtGt83GN7U",
        "outputId": "ecaaa822-2a9d-4aa7-ff64-c16f33df9613"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You : hello\n",
            "Bot:  hey that is me that is talking it\n"
          ]
        }
      ]
    }
  ]
}